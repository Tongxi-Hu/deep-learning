{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCx7MVg8R0JP"
      },
      "source": [
        "# Pytorch Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f35HQBiMR0JP"
      },
      "source": [
        "Pytorch is a python framework for machine learning\n",
        "\n",
        "- GPU-accelerated computations\n",
        "- automatic differentiation\n",
        "- modules for neural networks\n",
        "\n",
        "This tutorial will teach the fundamentals of operating on pytorch tensors and networks. You have already seen some things in recitation 0 which we will quickly review, but most of this tutorial is on mostly new or more advanced stuff.\n",
        "\n",
        "For a worked example of how to build and train a pytorch network, see `pytorch-example.ipynb`.\n",
        "\n",
        "For additional tutorials, see http://pytorch.org/tutorials/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O8Kk_QW2R0JQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5huPXEAtR0JQ",
        "outputId": "920918bc-5cec-47ec-9f10-972944e5c8d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "'1.10.2'"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTndjNDLR0JQ"
      },
      "source": [
        "## Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuhj2khUR0JQ"
      },
      "source": [
        "Tensors are the fundamental object for array data. The most common types you will use are `IntTensor` and `FloatTensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR1wEqHhR0JR",
        "outputId": "f0ea9702-3441-4be8-d6d8-1c8e372dccea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[7.5742e+31, 1.1576e+27, 4.5450e+30],\n",
            "        [1.8524e+28, 8.7721e-43, 2.2457e-40]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Create uninitialized tensor\n",
        "x = torch.FloatTensor(2,3)\n",
        "print(x)\n",
        "# Initialize to zeros\n",
        "x.zero_()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTOGTiF8R0JR",
        "outputId": "b1a183d9-ce51-420b-e911-c1404ba7022d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.6965, 0.2861, 0.2269],\n",
            "        [0.5513, 0.7195, 0.4231]])\n",
            "tensor([[0.6965, 0.2861, 0.2269],\n",
            "        [0.5513, 0.7195, 0.4231]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# Create from numpy array (seed for repeatability)\n",
        "np.random.seed(123)\n",
        "np_array = np.random.random((2,3))\n",
        "print(torch.FloatTensor(np_array))\n",
        "print(torch.from_numpy(np_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uMXCCJNxR0JR",
        "outputId": "8dc1e5a5-c431-438a-90ca-f7a6a7ccfb3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1115,  0.1204, -0.3696],\n",
            "        [-0.2404, -1.1969,  0.2093]])\n",
            "[[-0.11146712  0.12036294 -0.3696345 ]\n",
            " [-0.24041797 -1.1969243   0.20926936]]\n"
          ]
        }
      ],
      "source": [
        "# Create random tensor (seed for repeatability)\n",
        "torch.manual_seed(123)\n",
        "x=torch.randn(2,3)\n",
        "print(x)\n",
        "# export to numpy array\n",
        "x_np = x.numpy()\n",
        "print(x_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lhZFSQYR0JR",
        "outputId": "cedd4c80-9b54-45af-c8e1-5d524f3cc483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ],
      "source": [
        "# special tensors (see documentation)\n",
        "print(torch.eye(3))\n",
        "print(torch.ones(2,3))\n",
        "print(torch.zeros(2,3))\n",
        "print(torch.arange(0,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtPB0jrZR0JR"
      },
      "source": [
        "All tensors have a `size` and `type`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YefIpPWR0JR",
        "outputId": "6873deab-0de8-4421-a0a8-43a1c30961c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.FloatTensor\n"
          ]
        }
      ],
      "source": [
        "x=torch.FloatTensor(3,4)\n",
        "print(x.size())\n",
        "print(x.type())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEtTFmFR0JR"
      },
      "source": [
        "## Math, Linear Algebra, and Indexing (review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFhHPTY2R0JR"
      },
      "source": [
        "Pytorch math and linear algebra is similar to numpy. Operators are overridden so you can use standard math operators (`+`,`-`, etc.) and expect a tensor as a result. See pytorch documentation for a complete list of available functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S874vxjhR0JS",
        "outputId": "d8d24178-f45d-4674-efb5-69c5480ba120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(10.)\n",
            "tensor(85.7910)\n",
            "tensor(2.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(0.,5.)\n",
        "print(torch.sum(x))\n",
        "print(torch.sum(torch.exp(x)))\n",
        "print(torch.mean(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcJoWlwgR0JS"
      },
      "source": [
        "Pytorch indexing is similar to numpy indexing. See pytorch documentation for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNYNqaeQR0JS",
        "outputId": "3b7d016d-c139-44f5-88e3-2b69a3ee6838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n",
            "tensor([0.3164, 0.4017])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(3,2)\n",
        "print(x)\n",
        "print(x[1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE7dKpT4R0JS"
      },
      "source": [
        "## CPU and GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5vOx1gWR0JS"
      },
      "source": [
        "Tensors can be copied between CPU and GPU. It is important that everything involved in a calculation is on the same device.\n",
        "\n",
        "This portion of the tutorial may not work for you if you do not have a GPU available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKiVvnSOR0JS",
        "outputId": "6c5e59e1-4e3b-481b-b0aa-f2c12b1f81b4"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-1a729973c03d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# cannot get GPU tensor as numpy array directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ],
      "source": [
        "# create a tensor\n",
        "x = torch.rand(3,2)\n",
        "# copy to GPU\n",
        "y = x.cuda()\n",
        "# copy back to CPU\n",
        "z = y.cpu()\n",
        "# get CPU tensor as numpy array\n",
        "# cannot get GPU tensor as numpy array directly\n",
        "try:\n",
        "    y.numpy()\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA8PbSt8R0JS"
      },
      "source": [
        "Operations between GPU and CPU tensors will fail. Operations require all arguments to be on the same device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHA4Iox9R0JT"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(3,5)  # CPU tensor\n",
        "y = torch.rand(5,4).cuda()  # GPU tensor\n",
        "try:\n",
        "    torch.mm(x,y)  # Operation between CPU and GPU fails\n",
        "except TypeError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB4ZeyhcR0JT"
      },
      "source": [
        "Typical code should include `if` statements or utilize helper functions so it can operate with or without the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYZHzm0oR0JT",
        "outputId": "d28c618b-7339-4ac5-b6f1-4ec1311a3baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2745, 0.6584],\n",
            "        [0.2775, 0.8573],\n",
            "        [0.8993, 0.0390]], device='cuda:0') torch.float32\n",
            "tensor([[0.0753, 0.4335],\n",
            "        [0.0770, 0.7350],\n",
            "        [0.8088, 0.0015]], device='cuda:0')\n",
            "tensor([[0.0753, 0.4335],\n",
            "        [0.0770, 0.7350],\n",
            "        [0.8088, 0.0015]]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Put tensor on CUDA if available\n",
        "x = torch.rand(3,2)\n",
        "if torch.cuda.is_available():\n",
        "    x = x.cuda()\n",
        "    print(x, x.dtype)\n",
        "\n",
        "# Do some calculations\n",
        "y = x ** 2\n",
        "print(y)\n",
        "\n",
        "# Copy to CPU if on GPU\n",
        "if y.is_cuda:\n",
        "    y = y.cpu()\n",
        "    print(y, y.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhJWobER0JT"
      },
      "source": [
        "A convenient method is `new`, which creates a new tensor on the same device as another tensor. It should be used for creating tensors whenever possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqNk2BloR0JT",
        "outputId": "aad68040-076e-41b0-c9b0-77b14ce1d1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.]])\n",
            "tensor([[0.0753, 0.4335]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "x1 = torch.rand(3,2)\n",
        "x2 = x1.new(1,2)  # create cpu tensor\n",
        "print(x2)\n",
        "x1 = torch.rand(3,2).cuda()\n",
        "x2 = x1.new(1,2)  # create cuda tensor\n",
        "print(x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U43A1dfR0JT"
      },
      "source": [
        "Calculations executed on the GPU can be many times faster than numpy. However, numpy is still optimized for the CPU and many times faster than python `for` loops. Numpy calculations may be faster than GPU calculations for small arrays due to the cost of interfacing with the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZes7NGXR0JT"
      },
      "outputs": [],
      "source": [
        "from timeit import timeit\n",
        "# Create random data\n",
        "x = torch.rand(1000,64)\n",
        "y = torch.rand(64,32)\n",
        "number = 10000  # number of iterations\n",
        "\n",
        "def square():\n",
        "    z=torch.mm(x, y) # dot product (mm=matrix multiplication)\n",
        "\n",
        "# Time CPU\n",
        "print('CPU: {}ms'.format(timeit(square, number=number)*1000))\n",
        "# Time GPU\n",
        "x, y = x.cuda(), y.cuda()\n",
        "print('GPU: {}ms'.format(timeit(square, number=number)*1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqYCvSeqR0JT"
      },
      "source": [
        "## Differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgRHoh0FR0JT"
      },
      "source": [
        "Tensors provide automatic differentiation.\n",
        "\n",
        "As you might know, previous versions of Pytorch used Variables, which were wrappers around tensors for differentiation. Starting with pytorch 0.4.0, this wrapping is done internally in the Tensor class and you can, and should, differentiate Tensors directly. However, it is possible that you walk on references to Variables, e.g. in your error messages.\n",
        "\n",
        "What you need to remember :\n",
        "\n",
        "- Tensors you are differentiating with respect to must have `requires_grad=True`\n",
        "- Call `.backward()` on scalar variables you are differentiating\n",
        "- To differentiate a vector, sum it first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGBBdzaQR0JU",
        "outputId": "9d434e1a-f992-4c49-ba49-042807f52dc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.int64\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-13-aabc06498b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Calculate gradient (dy/dx=2x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# Print values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# Create differentiable tensor\n",
        "x = torch.tensor(torch.arange(0,4), requires_grad=False)\n",
        "print(x.dtype)\n",
        "# Calculate y=sum(x**2)\n",
        "y = x**2\n",
        "# Calculate gradient (dy/dx=2x)\n",
        "y.sum().backward()\n",
        "# Print values\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwDtZmNnR0JU"
      },
      "source": [
        "Differentiation accumulates gradients. This is sometimes what you want and sometimes not. **Make sure to zero gradients between batches if performing gradient descent or you will get strange results!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAyRPlc_R0JU",
        "outputId": "6bd095d5-df30-446f-e7de-7deb7aa53dd3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Only Tensors of floating point dtype can require gradients",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-14-3546ec580fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create a variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Differentiate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
          ]
        }
      ],
      "source": [
        "# Create a variable\n",
        "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
        "# Differentiate\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "# Differentiate again (accumulates gradient)\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "# Zero gradient before differentiating\n",
        "x.grad.data.zero_()\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Y3FP3fR0JU"
      },
      "source": [
        "Note that a Tensor with gradient cannot be exported to numpy directly :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3QlYRLJR0JU",
        "outputId": "e01ee9d4-4ab9-4e41-be59-5845c8ba4b21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Only Tensors of floating point dtype can require gradients",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-15-8c8213f31fb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# raises an exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
          ]
        }
      ],
      "source": [
        "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
        "x.numpy() # raises an exception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ngg-mmR0JU"
      },
      "source": [
        "The reason is that pytorch remembers the graph of all computations to perform differenciation. To be integrated to this graph the raw data is wrapped internally to the Tensor class (like what was formerly a Variable). You can detach the tensor from the graph using the **.detach()** method, which returns a tensor with the same data but requires_grad set to False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrW0ih2rR0JU",
        "outputId": "3fa6f868-8c6a-4695-bc72-5689ecb290c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Wendy\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Only Tensors of floating point dtype can require gradients",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-16-73751dbe633c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point dtype can require gradients"
          ]
        }
      ],
      "source": [
        "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
        "y=x**2\n",
        "z=y**2\n",
        "z.detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLYY0B7KR0JU"
      },
      "source": [
        "Another reason to use this method is that updating the graph can use a lot of memory. If you are in a context where you have a differentiable tensor that you don't need to differentiate, think of detaching it from the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "755ivvK_R0JU"
      },
      "source": [
        "## Neural Network Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBY2ISxuR0JU"
      },
      "source": [
        "Pytorch provides a framework for developing neural network modules. They take care of many things, the main one being wrapping and tracking a list of parameters for you.\n",
        "You have several ways of building and using a network, offering different tradeoffs between freedom and simplicity.\n",
        "\n",
        "torch.nn provides basic 1-layer nets, such as Linear (perceptron) and activation layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": false,
        "id": "FnWFLcaMR0JU",
        "outputId": "a05b9c83-a1cd-43c6-c448-0dc821bd5db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 14.6551, -21.2337,  11.2941,   2.4101,  -8.7841,  -8.4820,  -4.6494,\n",
            "         11.7071, -14.8966, -12.5824], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(0,32).float()\n",
        "net = torch.nn.Linear(32,10)\n",
        "y = net(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghZPhqw1R0JU"
      },
      "source": [
        "All nn.Module objects are reusable as components of bigger networks ! That is how you build personnalized nets. The simplest way is to use the nn.Sequential class.\n",
        "\n",
        "You can also create your own class that inherits n.Module. The forward method should precise what happens in the forward pass given an input. This enables you to precise behaviors more complicated than just applying layers one after another, if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1B_gm7OCR0JU"
      },
      "outputs": [],
      "source": [
        "# create a simple sequential network (`nn.Module` object) from layers (other `nn.Module` objects).\n",
        "# Here a MLP with 2 layers and sigmoid activation.\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(32,128),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Linear(128,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NfS8aRXoR0JU"
      },
      "outputs": [],
      "source": [
        "# create a more customizable network module (equivalent here)\n",
        "class MyNetwork(torch.nn.Module):\n",
        "    # you can use the layer sizes as initialization arguments if you want to\n",
        "    def __init__(self,input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
        "        self.layer2 = torch.nn.Sigmoid()\n",
        "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
        "\n",
        "    def forward(self, input_val):\n",
        "        h = input_val\n",
        "        h = self.layer1(h)\n",
        "        h = self.layer2(h)\n",
        "        h = self.layer3(h)\n",
        "        return h\n",
        "\n",
        "net = MyNetwork(32,128,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zak281ypR0JV"
      },
      "source": [
        "The network tracks parameters, and you can access them through the **parameters()** method, which returns a python generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ei0PLXwbR0JV",
        "outputId": "01ff34f6-70ac-4790-ccec-4eb12524ddb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0816, -0.0995, -0.1166,  ...,  0.1166,  0.0896,  0.0976],\n",
            "        [ 0.1667,  0.0878,  0.1543,  ...,  0.0377, -0.0838, -0.1021],\n",
            "        [ 0.0029,  0.1300,  0.0329,  ...,  0.0073,  0.0216, -0.1230],\n",
            "        ...,\n",
            "        [ 0.0614, -0.0652,  0.1360,  ...,  0.1361,  0.1089,  0.0750],\n",
            "        [ 0.1105,  0.0236, -0.1050,  ...,  0.0810,  0.0951, -0.1092],\n",
            "        [-0.1151, -0.0721,  0.0953,  ..., -0.0566,  0.1555,  0.0031]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 4.9386e-02, -5.2074e-02, -1.6802e-02,  4.3531e-02,  5.2061e-02,\n",
            "        -4.6065e-02,  4.8371e-02, -8.5445e-02,  1.6534e-01,  5.4044e-02,\n",
            "         1.3961e-01,  6.6039e-02,  3.2582e-02,  1.5769e-01,  7.1628e-02,\n",
            "         1.2248e-01,  2.9231e-03, -1.1006e-01, -1.1526e-01,  3.9579e-02,\n",
            "        -1.4732e-01, -8.8570e-02,  1.7245e-01, -1.2380e-01,  3.0472e-02,\n",
            "        -1.1035e-01,  3.1870e-02, -1.3361e-01,  1.6717e-01,  2.2847e-02,\n",
            "        -3.2548e-02, -8.8126e-02,  4.0940e-02,  2.0203e-02, -2.9818e-02,\n",
            "        -1.1327e-01, -6.2470e-02, -7.4502e-02,  1.4806e-01, -1.1850e-02,\n",
            "         1.4761e-01, -1.4352e-01, -1.3071e-01, -6.9670e-02,  1.1313e-01,\n",
            "         9.4507e-02,  6.4445e-02, -1.3196e-01,  7.8858e-02,  1.5638e-01,\n",
            "         8.4406e-02, -1.7449e-02,  2.2411e-02,  2.8611e-02, -3.1723e-02,\n",
            "        -1.4262e-01, -1.0461e-01, -1.1441e-01, -1.0294e-01,  4.7459e-02,\n",
            "         3.9379e-02, -1.7034e-01,  5.2169e-02, -1.8132e-02,  1.0058e-01,\n",
            "         2.2914e-02,  1.6526e-02, -1.5161e-01,  1.5219e-01,  8.1081e-02,\n",
            "        -1.0049e-01, -1.0209e-01,  1.2486e-01,  8.8050e-02,  8.1602e-02,\n",
            "        -1.1412e-01, -1.3023e-01,  6.6503e-02,  1.4920e-01,  1.1916e-01,\n",
            "         5.8697e-02, -1.1839e-01, -1.4722e-02, -3.4261e-02,  1.2925e-01,\n",
            "         1.5945e-01, -3.3697e-02,  7.4807e-02,  1.0326e-01,  1.3395e-01,\n",
            "        -1.6130e-01,  1.2642e-01,  1.6173e-01,  9.0355e-03,  3.8239e-02,\n",
            "         1.6079e-01,  2.3074e-02,  5.4586e-02,  3.5250e-03, -1.4497e-01,\n",
            "        -1.0516e-01,  9.7633e-02,  6.9292e-02,  3.7847e-02, -9.9425e-02,\n",
            "         2.8966e-02, -1.2854e-01,  1.4456e-01, -1.1698e-02, -5.7193e-02,\n",
            "        -5.6760e-02, -1.5453e-01, -1.0268e-01, -6.8814e-02, -1.1524e-01,\n",
            "         6.0484e-03, -4.2358e-05,  1.1993e-01,  1.1737e-01,  5.1905e-02,\n",
            "        -6.7531e-02, -4.9694e-02,  7.1119e-02, -8.8283e-02, -1.1980e-01,\n",
            "        -3.0280e-02, -7.2072e-02,  1.4048e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0515, -0.0549, -0.0192,  ...,  0.0221,  0.0065, -0.0395],\n",
            "        [-0.0287,  0.0802,  0.0809,  ..., -0.0081,  0.0037, -0.0262],\n",
            "        [-0.0012, -0.0767, -0.0543,  ..., -0.0515,  0.0684, -0.0876],\n",
            "        ...,\n",
            "        [ 0.0262, -0.0629,  0.0580,  ...,  0.0312, -0.0192,  0.0309],\n",
            "        [-0.0001, -0.0735,  0.0647,  ...,  0.0150, -0.0285, -0.0677],\n",
            "        [-0.0870, -0.0126, -0.0410,  ..., -0.0780,  0.0581, -0.0099]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0749, -0.0019, -0.0220,  0.0751, -0.0882, -0.0527, -0.0693, -0.0206,\n",
            "        -0.0521, -0.0831], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "for param in net.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BT1iizER0JV"
      },
      "source": [
        "Parameters are of type Parameter, which is basically a wrapper for a tensor. How does pytorch retrieve your network's parameters ? They are simply all the attributes of type Parameter in your network. Moreover, if an attribute is of type nn.Module, its own parameters are added to your network's parameters ! This is why, when you define a network by adding up basic components such as nn.Linear, you should never have to explicitely define parameters.\n",
        "\n",
        "However, if you are in a case where no pytorch default module does what you need, you can define parameters explicitely (this should be rare). For the record, let's build the previous MLP with personnalized parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ficoc--LR0JV"
      },
      "outputs": [],
      "source": [
        "class MyNetworkWithParams(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size, output_size):\n",
        "        super(MyNetworkWithParams,self).__init__()\n",
        "        self.layer1_weights = nn.Parameter(torch.randn(input_size,hidden_size))\n",
        "        self.layer1_bias = nn.Parameter(torch.randn(hidden_size))\n",
        "        self.layer2_weights = nn.Parameter(torch.randn(hidden_size,output_size))\n",
        "        self.layer2_bias = nn.Parameter(torch.randn(output_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        h1 = torch.matmul(x,self.layer1_weights) + self.layer1_bias\n",
        "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
        "        output = torch.matmul(h1_act,self.layer2_weights) + self.layer2_bias\n",
        "        return output\n",
        "\n",
        "net = MyNetworkWithParams(32,128,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPFTVmZXR0JV"
      },
      "source": [
        "Parameters are useful in that they are meant to be all the network's weights that will be optimized during training. If you were needing to use a tensor in your computational graph that you want to remain constant, just define it as a regular tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "426ufEB7R0JV"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bk99jlIZR0JV"
      },
      "outputs": [],
      "source": [
        "net = MyNetwork(32,128,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fW6Z7XPR0JV"
      },
      "source": [
        "The nn.Module also provides loss functions, such as cross-entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gdh3-x-JR0JV",
        "outputId": "c6ebfb59-6e5e-4ac0-98e3-0e2d909060f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.0366, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n",
        "y = torch.tensor([0,3,9])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "output = net(x)\n",
        "loss = criterion(output,y)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZEoLh-mR0JV"
      },
      "source": [
        "nn.CrossEntropyLoss does both the softmax and the actual cross-entropy : given $output$ of size $(n,d)$ and $y$ of size $n$ and values in $0,1,...,d-1$, it computes $\\sum_{i=0}^{n-1}log(s[i,y[i]])$ where $s[i,j] = \\frac{e^{output[i,j]}}{\\sum_{j'=0}^{d-1}e^{output[i,j']}}$\n",
        "\n",
        "You can also compose nn.LogSoftmax and nn.NLLLoss to get the same result. Note that all these use the log-softmax rather than the softmax, for stability in the computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t1MjHy5JR0JV",
        "outputId": "5ccc5258-313c-46c4-df9a-3257e659cf8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.0366, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# equivalent\n",
        "criterion2 = nn.NLLLoss()\n",
        "sf = nn.LogSoftmax()\n",
        "output = net(x)\n",
        "loss = criterion(sf(output),y)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ1jqDUDR0JV"
      },
      "source": [
        "Now, to perform the backward pass, just execute **loss.backward()** ! It will update gradients in all differentiable tensors in the graph, which in particular includes all the network parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "id": "nIN53SCrR0JV",
        "outputId": "4f8bbe50-52cb-4e7a-b59f-5310a9ac06d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0033, -0.0024, -0.0015,  ...,  0.0234,  0.0243,  0.0252],\n",
            "        [ 0.0083,  0.0083,  0.0083,  ...,  0.0083,  0.0083,  0.0083],\n",
            "        [-0.0052, -0.0052, -0.0052,  ..., -0.0052, -0.0052, -0.0052],\n",
            "        ...,\n",
            "        [-0.0066, -0.0066, -0.0066,  ..., -0.0070, -0.0070, -0.0070],\n",
            "        [-0.0008, -0.0030, -0.0052,  ..., -0.0659, -0.0682, -0.0704],\n",
            "        [-0.0016, -0.0016, -0.0016,  ..., -0.0015, -0.0015, -0.0015]])\n",
            "tensor([-5.9541e-03,  3.9260e-03, -1.0888e-02,  8.7817e-03, -1.3966e-03,\n",
            "         1.0375e-02,  2.5833e-03,  7.1461e-03,  1.0315e-03,  8.5597e-04,\n",
            "         3.5918e-03,  5.3241e-03,  4.0366e-03, -4.8465e-03, -7.4036e-03,\n",
            "        -6.8651e-03,  1.1752e-02, -2.6479e-03,  3.3971e-03, -2.9414e-03,\n",
            "         8.6274e-03, -5.6672e-03, -3.5430e-04, -2.8286e-03,  7.4300e-04,\n",
            "         1.8761e-03, -3.4793e-03, -2.4353e-03, -3.7101e-03, -1.0119e-03,\n",
            "        -4.3031e-03,  3.2497e-03,  2.3289e-04,  3.5754e-03, -3.7705e-03,\n",
            "         6.6169e-03,  4.7236e-03, -5.9270e-03,  1.4396e-03,  4.7972e-03,\n",
            "        -5.3177e-03,  5.1620e-03, -7.6672e-03, -4.3151e-04,  1.1066e-02,\n",
            "        -8.9210e-04,  6.0836e-03,  3.6081e-03,  1.0260e-03,  2.5124e-03,\n",
            "        -7.6811e-03, -4.3820e-03,  7.6409e-03, -2.2126e-03,  7.4655e-05,\n",
            "         2.6422e-03, -3.3749e-04, -4.6720e-04,  5.0239e-03,  4.4108e-03,\n",
            "        -8.8103e-03,  4.9034e-03,  8.6886e-04,  1.1605e-03,  6.2503e-03,\n",
            "         5.8412e-03, -2.4510e-03,  6.8243e-04, -2.7553e-03,  5.9913e-03,\n",
            "        -4.2024e-03,  7.9905e-03, -3.4927e-03, -7.4816e-03, -2.4823e-03,\n",
            "        -4.7119e-03, -5.1473e-03,  3.9038e-03, -1.1305e-03, -8.6300e-03,\n",
            "         1.6425e-03, -3.2428e-03,  7.3428e-03, -3.1141e-03,  2.9572e-04,\n",
            "         3.6881e-04, -2.3480e-03, -6.2566e-03,  2.7711e-03,  2.1197e-03,\n",
            "         9.3385e-04,  5.3948e-03, -5.2812e-03, -2.8810e-03,  1.9053e-03,\n",
            "        -4.7579e-03, -4.0161e-03,  1.1643e-03,  4.4605e-03, -5.9273e-03,\n",
            "        -6.5215e-03,  4.8277e-03,  2.0184e-03, -1.2082e-03, -6.4842e-04,\n",
            "        -2.9544e-04, -5.0835e-03,  1.4920e-03, -2.5653e-03,  2.9032e-03,\n",
            "         1.0384e-02, -7.6699e-03, -2.2206e-03,  1.0687e-02, -6.1140e-03,\n",
            "        -3.7211e-03,  9.0155e-03,  1.5268e-05,  2.0179e-03,  4.2890e-03,\n",
            "         5.7518e-03,  3.5493e-03, -7.0869e-03, -1.2209e-02, -1.1110e-03,\n",
            "        -7.2312e-03, -7.1841e-03, -3.9016e-03])\n",
            "tensor([[-0.1940,  0.0393,  0.0378,  ..., -0.2234, -0.1746, -0.2292],\n",
            "        [ 0.0418,  0.0203,  0.0195,  ...,  0.0485,  0.0402,  0.0458],\n",
            "        [ 0.0452,  0.0211,  0.0203,  ...,  0.0527,  0.0434,  0.0497],\n",
            "        ...,\n",
            "        [ 0.0764,  0.0402,  0.0390,  ...,  0.0885,  0.0738,  0.0832],\n",
            "        [ 0.0837,  0.0474,  0.0460,  ...,  0.0967,  0.0811,  0.0905],\n",
            "        [-0.1064, -0.1167, -0.0966,  ..., -0.1515, -0.1067, -0.1212]])\n",
            "tensor([-0.1849,  0.0686,  0.0736, -0.1666,  0.0615,  0.0473,  0.0831,  0.1285,\n",
            "         0.1440, -0.2551])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "\n",
        "# Check that the parameters now have gradients\n",
        "for param in net.parameters():\n",
        "    print(param.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "Q47X0HFKR0JY",
        "outputId": "cb1f8890-6d0c-4d9d-8e9e-24a66a4307e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0067, -0.0048, -0.0030,  ...,  0.0467,  0.0485,  0.0504],\n",
            "        [ 0.0165,  0.0165,  0.0165,  ...,  0.0165,  0.0165,  0.0165],\n",
            "        [-0.0104, -0.0104, -0.0104,  ..., -0.0104, -0.0104, -0.0104],\n",
            "        ...,\n",
            "        [-0.0132, -0.0132, -0.0133,  ..., -0.0140, -0.0140, -0.0141],\n",
            "        [-0.0015, -0.0060, -0.0105,  ..., -0.1319, -0.1363, -0.1408],\n",
            "        [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]])\n",
            "tensor([-1.1908e-02,  7.8520e-03, -2.1776e-02,  1.7563e-02, -2.7932e-03,\n",
            "         2.0751e-02,  5.1667e-03,  1.4292e-02,  2.0631e-03,  1.7119e-03,\n",
            "         7.1836e-03,  1.0648e-02,  8.0732e-03, -9.6931e-03, -1.4807e-02,\n",
            "        -1.3730e-02,  2.3504e-02, -5.2957e-03,  6.7943e-03, -5.8828e-03,\n",
            "         1.7255e-02, -1.1334e-02, -7.0861e-04, -5.6573e-03,  1.4860e-03,\n",
            "         3.7523e-03, -6.9587e-03, -4.8705e-03, -7.4203e-03, -2.0238e-03,\n",
            "        -8.6062e-03,  6.4995e-03,  4.6577e-04,  7.1508e-03, -7.5410e-03,\n",
            "         1.3234e-02,  9.4472e-03, -1.1854e-02,  2.8793e-03,  9.5943e-03,\n",
            "        -1.0635e-02,  1.0324e-02, -1.5334e-02, -8.6303e-04,  2.2132e-02,\n",
            "        -1.7842e-03,  1.2167e-02,  7.2163e-03,  2.0520e-03,  5.0247e-03,\n",
            "        -1.5362e-02, -8.7640e-03,  1.5282e-02, -4.4252e-03,  1.4931e-04,\n",
            "         5.2843e-03, -6.7498e-04, -9.3439e-04,  1.0048e-02,  8.8217e-03,\n",
            "        -1.7621e-02,  9.8068e-03,  1.7377e-03,  2.3210e-03,  1.2501e-02,\n",
            "         1.1682e-02, -4.9020e-03,  1.3649e-03, -5.5107e-03,  1.1983e-02,\n",
            "        -8.4048e-03,  1.5981e-02, -6.9854e-03, -1.4963e-02, -4.9646e-03,\n",
            "        -9.4237e-03, -1.0295e-02,  7.8077e-03, -2.2611e-03, -1.7260e-02,\n",
            "         3.2851e-03, -6.4857e-03,  1.4686e-02, -6.2282e-03,  5.9144e-04,\n",
            "         7.3762e-04, -4.6959e-03, -1.2513e-02,  5.5422e-03,  4.2393e-03,\n",
            "         1.8677e-03,  1.0790e-02, -1.0562e-02, -5.7621e-03,  3.8106e-03,\n",
            "        -9.5158e-03, -8.0322e-03,  2.3285e-03,  8.9209e-03, -1.1855e-02,\n",
            "        -1.3043e-02,  9.6555e-03,  4.0368e-03, -2.4163e-03, -1.2968e-03,\n",
            "        -5.9087e-04, -1.0167e-02,  2.9841e-03, -5.1305e-03,  5.8063e-03,\n",
            "         2.0768e-02, -1.5340e-02, -4.4411e-03,  2.1373e-02, -1.2228e-02,\n",
            "        -7.4422e-03,  1.8031e-02,  3.0536e-05,  4.0359e-03,  8.5781e-03,\n",
            "         1.1504e-02,  7.0987e-03, -1.4174e-02, -2.4418e-02, -2.2220e-03,\n",
            "        -1.4462e-02, -1.4368e-02, -7.8032e-03])\n",
            "tensor([[-0.3881,  0.0786,  0.0756,  ..., -0.4469, -0.3492, -0.4584],\n",
            "        [ 0.0836,  0.0405,  0.0391,  ...,  0.0971,  0.0804,  0.0916],\n",
            "        [ 0.0905,  0.0423,  0.0405,  ...,  0.1053,  0.0868,  0.0994],\n",
            "        ...,\n",
            "        [ 0.1529,  0.0805,  0.0780,  ...,  0.1769,  0.1475,  0.1663],\n",
            "        [ 0.1674,  0.0948,  0.0921,  ...,  0.1933,  0.1621,  0.1810],\n",
            "        [-0.2127, -0.2335, -0.1933,  ..., -0.3029, -0.2135, -0.2424]])\n",
            "tensor([-0.3698,  0.1373,  0.1471, -0.3332,  0.1230,  0.0946,  0.1662,  0.2571,\n",
            "         0.2879, -0.5102])\n",
            "tensor([[-0.0033, -0.0024, -0.0015,  ...,  0.0234,  0.0243,  0.0252],\n",
            "        [ 0.0083,  0.0083,  0.0083,  ...,  0.0083,  0.0083,  0.0083],\n",
            "        [-0.0052, -0.0052, -0.0052,  ..., -0.0052, -0.0052, -0.0052],\n",
            "        ...,\n",
            "        [-0.0066, -0.0066, -0.0066,  ..., -0.0070, -0.0070, -0.0070],\n",
            "        [-0.0008, -0.0030, -0.0052,  ..., -0.0659, -0.0682, -0.0704],\n",
            "        [-0.0016, -0.0016, -0.0016,  ..., -0.0015, -0.0015, -0.0015]])\n",
            "tensor([-5.9541e-03,  3.9260e-03, -1.0888e-02,  8.7817e-03, -1.3966e-03,\n",
            "         1.0375e-02,  2.5833e-03,  7.1461e-03,  1.0315e-03,  8.5597e-04,\n",
            "         3.5918e-03,  5.3241e-03,  4.0366e-03, -4.8465e-03, -7.4036e-03,\n",
            "        -6.8651e-03,  1.1752e-02, -2.6479e-03,  3.3971e-03, -2.9414e-03,\n",
            "         8.6274e-03, -5.6672e-03, -3.5430e-04, -2.8286e-03,  7.4300e-04,\n",
            "         1.8761e-03, -3.4793e-03, -2.4353e-03, -3.7101e-03, -1.0119e-03,\n",
            "        -4.3031e-03,  3.2497e-03,  2.3289e-04,  3.5754e-03, -3.7705e-03,\n",
            "         6.6169e-03,  4.7236e-03, -5.9270e-03,  1.4396e-03,  4.7972e-03,\n",
            "        -5.3177e-03,  5.1620e-03, -7.6672e-03, -4.3151e-04,  1.1066e-02,\n",
            "        -8.9210e-04,  6.0836e-03,  3.6081e-03,  1.0260e-03,  2.5124e-03,\n",
            "        -7.6811e-03, -4.3820e-03,  7.6409e-03, -2.2126e-03,  7.4655e-05,\n",
            "         2.6422e-03, -3.3749e-04, -4.6720e-04,  5.0239e-03,  4.4108e-03,\n",
            "        -8.8103e-03,  4.9034e-03,  8.6886e-04,  1.1605e-03,  6.2503e-03,\n",
            "         5.8412e-03, -2.4510e-03,  6.8243e-04, -2.7553e-03,  5.9913e-03,\n",
            "        -4.2024e-03,  7.9905e-03, -3.4927e-03, -7.4816e-03, -2.4823e-03,\n",
            "        -4.7119e-03, -5.1473e-03,  3.9038e-03, -1.1305e-03, -8.6300e-03,\n",
            "         1.6425e-03, -3.2428e-03,  7.3428e-03, -3.1141e-03,  2.9572e-04,\n",
            "         3.6881e-04, -2.3480e-03, -6.2566e-03,  2.7711e-03,  2.1197e-03,\n",
            "         9.3385e-04,  5.3948e-03, -5.2812e-03, -2.8810e-03,  1.9053e-03,\n",
            "        -4.7579e-03, -4.0161e-03,  1.1643e-03,  4.4605e-03, -5.9273e-03,\n",
            "        -6.5215e-03,  4.8277e-03,  2.0184e-03, -1.2082e-03, -6.4842e-04,\n",
            "        -2.9544e-04, -5.0835e-03,  1.4920e-03, -2.5653e-03,  2.9032e-03,\n",
            "         1.0384e-02, -7.6699e-03, -2.2206e-03,  1.0687e-02, -6.1140e-03,\n",
            "        -3.7211e-03,  9.0155e-03,  1.5268e-05,  2.0179e-03,  4.2890e-03,\n",
            "         5.7518e-03,  3.5493e-03, -7.0869e-03, -1.2209e-02, -1.1110e-03,\n",
            "        -7.2312e-03, -7.1841e-03, -3.9016e-03])\n",
            "tensor([[-0.1940,  0.0393,  0.0378,  ..., -0.2234, -0.1746, -0.2292],\n",
            "        [ 0.0418,  0.0203,  0.0195,  ...,  0.0485,  0.0402,  0.0458],\n",
            "        [ 0.0452,  0.0211,  0.0203,  ...,  0.0527,  0.0434,  0.0497],\n",
            "        ...,\n",
            "        [ 0.0764,  0.0402,  0.0390,  ...,  0.0885,  0.0738,  0.0832],\n",
            "        [ 0.0837,  0.0474,  0.0460,  ...,  0.0967,  0.0811,  0.0905],\n",
            "        [-0.1064, -0.1167, -0.0966,  ..., -0.1515, -0.1067, -0.1212]])\n",
            "tensor([-0.1849,  0.0686,  0.0736, -0.1666,  0.0615,  0.0473,  0.0831,  0.1285,\n",
            "         0.1440, -0.2551])\n"
          ]
        }
      ],
      "source": [
        "# if I forward prop and backward prop again, gradients accumulate :\n",
        "output = net(x)\n",
        "loss = criterion(output,y)\n",
        "loss.backward()\n",
        "for param in net.parameters():\n",
        "    print(param.grad)\n",
        "\n",
        "# you can remove this behavior by reinitializing the gradients in your network parameters :\n",
        "net.zero_grad()\n",
        "output = net(x)\n",
        "loss = criterion(output,y)\n",
        "loss.backward()\n",
        "for param in net.parameters():\n",
        "    print(param.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtD0ArmrR0JY"
      },
      "source": [
        "We did backpropagation, but still didn't perform gradient descent. Let's define an optimizer on the network parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "id": "8tzgTEIWR0JY",
        "outputId": "9156c2c2-a195-411e-88a9-107884d99fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters before gradient descent :\n",
            "Parameter containing:\n",
            "tensor([[-0.1029, -0.1547, -0.1360,  ..., -0.0023, -0.1131,  0.0784],\n",
            "        [ 0.1237, -0.1399,  0.0348,  ...,  0.0808, -0.0545, -0.1730],\n",
            "        [ 0.1708,  0.0114, -0.0192,  ...,  0.1465, -0.1132, -0.1625],\n",
            "        ...,\n",
            "        [-0.0782,  0.1179,  0.0345,  ..., -0.1578, -0.0492, -0.1509],\n",
            "        [-0.1569, -0.0867,  0.0415,  ..., -0.1076, -0.0669, -0.0068],\n",
            "        [-0.0400,  0.0671,  0.1368,  ..., -0.0625,  0.1141,  0.1302]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0537, -0.0661,  0.0626,  0.1646,  0.0605, -0.1666,  0.0463,  0.0664,\n",
            "        -0.0817,  0.1747, -0.0230, -0.0043,  0.1013, -0.1205, -0.0213, -0.0446,\n",
            "         0.1154, -0.1111,  0.1758,  0.0344, -0.0786,  0.1729,  0.1386,  0.0203,\n",
            "         0.1017, -0.0405, -0.1326,  0.1548, -0.0391, -0.1304,  0.0471, -0.0857,\n",
            "         0.1218, -0.0449, -0.1258,  0.1711, -0.0497, -0.1714, -0.1058, -0.0260,\n",
            "         0.1496, -0.1583, -0.0245, -0.0573, -0.0696, -0.0556, -0.0168,  0.1435,\n",
            "         0.0414,  0.1160,  0.1302,  0.0431, -0.1731, -0.0029, -0.1268,  0.1425,\n",
            "        -0.1547, -0.1727,  0.1297, -0.1200, -0.0192, -0.0015, -0.1674, -0.0354,\n",
            "        -0.0686, -0.0215,  0.1345, -0.0038, -0.1437,  0.0038,  0.1523,  0.0774,\n",
            "        -0.1232, -0.1333, -0.1645, -0.0522,  0.0330,  0.0889, -0.1763, -0.1641,\n",
            "        -0.1653, -0.0737,  0.0544, -0.1281, -0.0041, -0.0480, -0.1531,  0.0143,\n",
            "        -0.0008,  0.0141, -0.0877, -0.0973,  0.0362, -0.0418,  0.1573,  0.1330,\n",
            "        -0.0045, -0.0815,  0.1406,  0.1226, -0.1304,  0.1116, -0.0489, -0.0970,\n",
            "        -0.0406, -0.1695,  0.1590,  0.0574, -0.0142, -0.0163,  0.1642,  0.1231,\n",
            "        -0.0792, -0.0017,  0.1321, -0.0564, -0.0596, -0.0161, -0.1471, -0.0335,\n",
            "         0.1625, -0.0514,  0.1452, -0.0486, -0.0797, -0.0665, -0.0254, -0.1544],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0100,  0.0455, -0.0142,  ...,  0.0416,  0.0487, -0.0127],\n",
            "        [-0.0391, -0.0040, -0.0178,  ..., -0.0188, -0.0280, -0.0739],\n",
            "        [-0.0259, -0.0012, -0.0543,  ..., -0.0763, -0.0647, -0.0116],\n",
            "        ...,\n",
            "        [ 0.0347,  0.0506, -0.0796,  ...,  0.0650,  0.0646, -0.0169],\n",
            "        [ 0.0669,  0.0788,  0.0288,  ..., -0.0329, -0.0747, -0.0668],\n",
            "        [ 0.0553, -0.0709,  0.0735,  ...,  0.0880,  0.0119,  0.0068]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0714, -0.0012, -0.0278,  0.0864, -0.0853,  0.0544,  0.0145, -0.0632,\n",
            "         0.0721, -0.0683], requires_grad=True)\n",
            "Parameters after gradient descent :\n",
            "Parameter containing:\n",
            "tensor([[-0.1029, -0.1547, -0.1360,  ..., -0.0025, -0.1133,  0.0781],\n",
            "        [ 0.1236, -0.1400,  0.0347,  ...,  0.0808, -0.0545, -0.1731],\n",
            "        [ 0.1708,  0.0115, -0.0192,  ...,  0.1466, -0.1131, -0.1624],\n",
            "        ...,\n",
            "        [-0.0781,  0.1180,  0.0346,  ..., -0.1577, -0.0491, -0.1508],\n",
            "        [-0.1569, -0.0866,  0.0416,  ..., -0.1070, -0.0663, -0.0061],\n",
            "        [-0.0399,  0.0671,  0.1368,  ..., -0.0624,  0.1141,  0.1302]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0537, -0.0662,  0.0627,  0.1645,  0.0605, -0.1667,  0.0463,  0.0663,\n",
            "        -0.0817,  0.1747, -0.0230, -0.0043,  0.1013, -0.1205, -0.0212, -0.0445,\n",
            "         0.1152, -0.1111,  0.1757,  0.0344, -0.0787,  0.1730,  0.1386,  0.0203,\n",
            "         0.1017, -0.0405, -0.1326,  0.1548, -0.0391, -0.1303,  0.0471, -0.0857,\n",
            "         0.1218, -0.0449, -0.1258,  0.1710, -0.0498, -0.1714, -0.1059, -0.0260,\n",
            "         0.1497, -0.1584, -0.0244, -0.0573, -0.0697, -0.0556, -0.0169,  0.1435,\n",
            "         0.0414,  0.1160,  0.1303,  0.0432, -0.1732, -0.0029, -0.1268,  0.1425,\n",
            "        -0.1547, -0.1727,  0.1297, -0.1200, -0.0191, -0.0015, -0.1674, -0.0354,\n",
            "        -0.0686, -0.0216,  0.1345, -0.0038, -0.1436,  0.0038,  0.1524,  0.0773,\n",
            "        -0.1231, -0.1332, -0.1645, -0.0522,  0.0331,  0.0889, -0.1763, -0.1640,\n",
            "        -0.1653, -0.0737,  0.0543, -0.1281, -0.0041, -0.0480, -0.1530,  0.0143,\n",
            "        -0.0009,  0.0141, -0.0877, -0.0974,  0.0363, -0.0417,  0.1573,  0.1331,\n",
            "        -0.0045, -0.0815,  0.1405,  0.1227, -0.1304,  0.1116, -0.0489, -0.0969,\n",
            "        -0.0406, -0.1695,  0.1590,  0.0574, -0.0142, -0.0163,  0.1641,  0.1232,\n",
            "        -0.0791, -0.0018,  0.1321, -0.0563, -0.0597, -0.0161, -0.1471, -0.0336,\n",
            "         0.1624, -0.0514,  0.1452, -0.0485, -0.0797, -0.0664, -0.0253, -0.1544],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0081,  0.0451, -0.0146,  ...,  0.0439,  0.0504, -0.0104],\n",
            "        [-0.0395, -0.0042, -0.0180,  ..., -0.0193, -0.0284, -0.0743],\n",
            "        [-0.0264, -0.0014, -0.0545,  ..., -0.0768, -0.0651, -0.0121],\n",
            "        ...,\n",
            "        [ 0.0340,  0.0502, -0.0800,  ...,  0.0641,  0.0639, -0.0177],\n",
            "        [ 0.0661,  0.0784,  0.0283,  ..., -0.0338, -0.0756, -0.0677],\n",
            "        [ 0.0563, -0.0697,  0.0744,  ...,  0.0896,  0.0130,  0.0080]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0733, -0.0018, -0.0286,  0.0881, -0.0859,  0.0539,  0.0137, -0.0645,\n",
            "         0.0707, -0.0657], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Parameters before gradient descent :\")\n",
        "for param in net.parameters():\n",
        "    print(param)\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Parameters after gradient descent :\")\n",
        "for param in net.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYocRdlUR0JY",
        "outputId": "9dbd40e9-0bfc-4d73-d533-3d2958ec5c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.0752, grad_fn=<NllLossBackward>)\n",
            "tensor(1.9747, grad_fn=<NllLossBackward>)\n",
            "tensor(1.8850, grad_fn=<NllLossBackward>)\n",
            "tensor(1.8045, grad_fn=<NllLossBackward>)\n",
            "tensor(1.7319, grad_fn=<NllLossBackward>)\n",
            "tensor(1.6661, grad_fn=<NllLossBackward>)\n",
            "tensor(1.6066, grad_fn=<NllLossBackward>)\n",
            "tensor(1.5526, grad_fn=<NllLossBackward>)\n",
            "tensor(1.5034, grad_fn=<NllLossBackward>)\n",
            "tensor(1.4583, grad_fn=<NllLossBackward>)\n",
            "tensor(1.4168, grad_fn=<NllLossBackward>)\n",
            "tensor(1.3782, grad_fn=<NllLossBackward>)\n",
            "tensor(1.3424, grad_fn=<NllLossBackward>)\n",
            "tensor(1.3088, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2773, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2477, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2197, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1934, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1685, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1450, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1228, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1017, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0818, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0628, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0447, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0275, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9954, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9523, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9142, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8596, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8497, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8310, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8221, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8135, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8051, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7891, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7596, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7528, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7461, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7332, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7209, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6926, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6874, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6771, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6673, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6626, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6579, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6534, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6445, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6318, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6277, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6237, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6198, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6159, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6084, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6047, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6011, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5975, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5618, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5559, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5502, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5474, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5418, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5338, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5312, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5287, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5236, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5139, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5115, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5046, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4957, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4935, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4830, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4749, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4730, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4672, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4653, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4634, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4579, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4561, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4543, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4507, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4455, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4438, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4404, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4387, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4355, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4338, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4274, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4259, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4227, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4212, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4197, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4182, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4078, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3994, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3926, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3821, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3795, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3783, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3770, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3745, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3696, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3672, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3636, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3590, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3578, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3533, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3522, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3510, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3455, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3445, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3434, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3412, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3339, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3329, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3318, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3278, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3209, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3199, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3189, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3180, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3170, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3161, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3142, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3132, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3104, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3095, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3077, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3058, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3013, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2996, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2987, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2935, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2926, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2918, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2901, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2843, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2834, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2818, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2810, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2794, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2778, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2770, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2739, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2723, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2715, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2700, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2692, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2677, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2669, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2662, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2647, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2640, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2617, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2610, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2603, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2595, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2581, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2560, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2552, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2531, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2524, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2517, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2510, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2503, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2496, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2483, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2469, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2462, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2456, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2435, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2429, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2422, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2416, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2409, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2389, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2383, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2376, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2370, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2363, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2357, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2351, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2344, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2338, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2332, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2325, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2313, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2307, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2294, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2276, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2252, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2246, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2240, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2234, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2216, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2210, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2199, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2181, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2170, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2164, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2153, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2147, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2141, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2136, grad_fn=<NllLossBackward>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.2130, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2119, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2097, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2054, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2044, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2038, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2028, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2007, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1987, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1982, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1977, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1972, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1957, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1927, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1922, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1893, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1879, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1861, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1856, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1842, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1824, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1820, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1797, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1784, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1780, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1771, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1767, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1750, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1712, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1700, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1696, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1692, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1688, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1676, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1672, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1668, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1656, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1652, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1640, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1636, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1629, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1621, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1617, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1610, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1606, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1595, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1587, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1580, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1573, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1569, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1540, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1533, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1523, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1520, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1516, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1513, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1509, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1506, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1502, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1496, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1492, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1486, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1482, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1479, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1469, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1462, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1459, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1456, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1453, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1437, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1433, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1430, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1427, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1424, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1418, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1415, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1408, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1405, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1393, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1387, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1375, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1372, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1366, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1363, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1357, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1351, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1346, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1340, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1334, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1331, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1329, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1323, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1320, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1315, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1312, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1303, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1298, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1287, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1284, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1279, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1276, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1274, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1268, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1266, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1263, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1255, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1253, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1240, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1237, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1232, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1230, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1227, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1225, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1220, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1213, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1210, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1208, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1203, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1198, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1195, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1191, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1188, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1184, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1181, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1179, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1177, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1174, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1170, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1165, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1156, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1153, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1147, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1144, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1142, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1138, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1135, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1133, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1131, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1129, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1127, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1124, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1120, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1118, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1116, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1101, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1099, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1097, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1095, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1090, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1088, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1084, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1080, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1078, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1074, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1072, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1068, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1066, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1062, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1060, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1058, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1054, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1052, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1048, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1046, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1044, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1038, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1012, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1006, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1003, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0999, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0996, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0994, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0990, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0988, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0987, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0985, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0981, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0974, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0971, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0964, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0957, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0955, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0954, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0944, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0929, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0928, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0926, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0924, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0923, grad_fn=<NllLossBackward>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0921, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0920, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0918, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0916, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0904, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0901, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0896, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0895, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0893, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0883, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0880, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0878, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0874, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0873, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0864, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0863, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0861, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0855, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0854, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0848, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0843, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0842, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0837, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0832, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0831, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0828, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0824, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0821, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0820, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0819, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0818, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0816, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0810, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0807, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0801, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0800, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0797, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0795, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0791, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0790, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0787, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0785, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0783, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0782, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0780, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0777, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0775, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0771, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0770, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0768, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0766, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0756, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0752, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0751, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0749, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0748, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0747, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0745, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0744, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0743, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0739, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0736, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0734, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0732, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0730, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0727, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0724, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0723, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0719, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0717, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0715, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0713, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0712, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0711, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0709, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0706, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0705, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0702, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0700, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0699, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0698, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0696, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0694, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0693, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0692, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0690, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0688, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0687, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0683, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0682, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0681, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0677, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0676, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0675, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0674, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0673, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0672, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0670, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0669, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0668, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0666, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0665, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0662, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0659, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0657, grad_fn=<NllLossBackward>)\n"
          ]
        }
      ],
      "source": [
        "# In a training loop, we should perform many GD iterations.\n",
        "n_iter = 1000\n",
        "for i in range(n_iter):\n",
        "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
        "    output = net(x)\n",
        "    loss = criterion(output,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Haa0u2opR0JY",
        "outputId": "1caafd2e-1a3f-4364-f636-f2784e026d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 7.9902, -1.7800, -2.0774,  0.5787, -1.9603, -1.7135, -1.6187, -1.8039,\n",
            "         -1.8273,  4.1082],\n",
            "        [ 0.2567, -1.4705, -1.5120,  5.7903, -1.5307, -1.6327, -1.5679, -1.5251,\n",
            "         -1.6856,  3.2233],\n",
            "        [ 1.8633, -1.4923, -1.5976,  3.0309, -1.6929, -1.5278, -1.5327, -1.6354,\n",
            "         -1.6566,  5.6728]], grad_fn=<AddmmBackward>)\n",
            "tensor([0, 3, 9])\n"
          ]
        }
      ],
      "source": [
        "output = net(x)\n",
        "print(output)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQFShWUTR0JY"
      },
      "source": [
        "Now you know how to train a network ! For a complete training check the pytorch_example notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "all in one block:"
      ],
      "metadata": {
        "id": "beikQAMnTmO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "x=torch.tensor([np.arange(32),np.zeros(32),np.ones(32)]).float()\n",
        "y=torch.tensor([0,3,9])\n",
        "net=nn.Sequential(nn.Linear(32,128),nn.ReLU(),nn.Linear(128,10))\n",
        "critierion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output=net(x)\n",
        "    loss=criterion(output,y)\n",
        "    loss.backward()\n",
        "    print(loss)\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "i5lW1iiiTlBU",
        "outputId": "92d1ac57-d9e3-48ad-9715-f6d52efcdff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.1786, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.0213, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4124, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2926, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0431, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8928, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8029, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7822, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7105, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6862, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9645, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6285, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6016, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5786, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5574, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5375, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5186, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5007, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4666, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4501, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4341, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4184, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4025, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3873, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3723, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3577, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3433, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3293, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3157, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3025, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2897, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2773, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2653, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2537, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2426, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2319, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2216, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2118, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2023, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1934, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1848, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1767, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1689, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1615, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1545, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1479, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1416, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1356, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1300, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1247, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1196, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1148, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1103, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1060, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1019, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0981, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0944, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0910, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0877, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0846, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0816, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0789, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0762, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0737, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0713, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0690, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0668, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0648, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0628, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0609, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0592, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0575, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0558, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0543, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0528, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0514, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0500, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0487, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0474, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0463, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0451, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0440, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0429, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0419, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0409, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0400, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0391, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0382, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0374, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0366, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0358, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0350, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0343, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0336, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0329, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0323, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0317, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0310, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0304, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMaYTt-HR0JY"
      },
      "source": [
        "## Saving and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_q22LrUR0JY",
        "outputId": "bf48ce54-1087-4f76-9a86-088b6a0d743c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n"
          ]
        }
      ],
      "source": [
        "# get dictionary of keys to weights using `state_dict`\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(28*28,256),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Linear(256,10))\n",
        "print(net.state_dict().keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byppFSCZR0JY",
        "outputId": "70ddcfae-59e1-4156-a055-84b8c3f33331"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# save a dictionary\n",
        "torch.save(net.state_dict(),'test.t7')\n",
        "# load a dictionary\n",
        "net.load_state_dict(torch.load('test.t7'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct86PhubR0JY"
      },
      "source": [
        "## Common issues to look out for"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WNx5kuER0JY"
      },
      "source": [
        "### Type mismatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hKzz-yjhR0JY",
        "outputId": "aa26149d-68fa-4c3f-aa2b-864698a30255"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-32-4d1c8f2c4847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'"
          ]
        }
      ],
      "source": [
        "net = nn.Linear(4,2)\n",
        "x = torch.tensor([1,2,3,4])\n",
        "y = net(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtXOhbLlR0JY"
      },
      "outputs": [],
      "source": [
        "x = x.float()\n",
        "x = torch.tensor([1.,2.,3.,4.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9ODS_HanR0JY",
        "outputId": "ddb89e37-e27a-4c0d-f45f-b5b7e2c5e5e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[6., 6.],\n",
            "        [6., 6.]])\n",
            "tensor([[12., 12.],\n",
            "        [12., 12.]])\n"
          ]
        }
      ],
      "source": [
        "x = 2* torch.ones(2,2)\n",
        "y = 3* torch.ones(2,2)\n",
        "print(x * y)\n",
        "print(x.matmul(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk5GqjqTR0JY",
        "outputId": "b94f4fc1-1c38-4916-c67b-50d36df8ce9b"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "expected device cpu and dtype Float but got device cpu and dtype Long",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-35-d8d146283c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: expected device cpu and dtype Float but got device cpu and dtype Long"
          ]
        }
      ],
      "source": [
        "x = torch.ones(4,5)\n",
        "y = torch.arange(5)\n",
        "print(x+y)\n",
        "y = torch.arange(4).view(-1,1)\n",
        "print(x+y)\n",
        "y = torch.arange(4)\n",
        "print(x+y) # exception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr_L9eCfR0JZ",
        "outputId": "5160a3a1-e425-48b5-fe2c-037b384f5f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(x)\n",
        "print(x.t())\n",
        "print(x.view(3,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnBZdj_2R0JZ",
        "outputId": "35ed0ad6-0e07-4159-d72c-731b769b82d6"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected object of backend CUDA but got backend CPU for argument #4 'mat1'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-37-7eb9531af49d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #4 'mat1'"
          ]
        }
      ],
      "source": [
        "net = nn.Sequential(nn.Linear(2048,2048),nn.ReLU(),\n",
        "                   nn.Linear(2048,2048),nn.ReLU(),\n",
        "                   nn.Linear(2048,2048),nn.ReLU(),\n",
        "                   nn.Linear(2048,2048),nn.ReLU(),\n",
        "                   nn.Linear(2048,2048),nn.ReLU(),\n",
        "                   nn.Linear(2048,2048),nn.ReLU(),\n",
        "                   nn.Linear(2048,120))\n",
        "x = torch.ones(256,2048)\n",
        "y = torch.zeros(256).long()\n",
        "net.cuda()\n",
        "x.cuda()\n",
        "crit=nn.CrossEntropyLoss()\n",
        "out = net(x)\n",
        "loss = crit(out,y)\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzyn_H5KR0JZ"
      },
      "outputs": [],
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self,n_hidden_layers):\n",
        "        super(MyNet,self).__init__()\n",
        "        self.n_hidden_layers=n_hidden_layers\n",
        "        self.final_layer = nn.Linear(128,10)\n",
        "        self.act = nn.ReLU()\n",
        "        self.hidden = []\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.hidden.append(nn.Linear(128,128))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        h = x\n",
        "        for i in range(self.n_hidden_layers):\n",
        "            h = self.hidden[i](h)\n",
        "            h = self.act(h)\n",
        "        out = self.final_layer(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGazOIsOR0JZ"
      },
      "outputs": [],
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self,n_hidden_layers):\n",
        "        super(MyNet,self).__init__()\n",
        "        self.n_hidden_layers=n_hidden_layers\n",
        "        self.final_layer = nn.Linear(128,10)\n",
        "        self.act = nn.ReLU()\n",
        "        self.hidden = []\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.hidden.append(nn.Linear(128,128))\n",
        "        self.hidden = nn.ModuleList(self.hidden)\n",
        "\n",
        "    def forward(self,x):\n",
        "        h = x\n",
        "        for i in range(self.n_hidden_layers):\n",
        "            h = self.hidden[i](h)\n",
        "            h = self.act(h)\n",
        "        out = self.final_layer(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ2dXpwOR0JZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3910jvsc74a57bd0b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e",
      "language": "python",
      "display_name": "Python 3.9.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Tutorial-pytorch_F19.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}